<!doctype html><html lang=en-us><head><title>Jenkins on Azure Kubernetes Cluster (AKS) - How to build container images with BuildKit, Jib, Kaniko and use Azure Files NFS, Azure Container Registry (ACR) and Azure Blob storage for caching // Rafal Hollins</title><link rel="shortcut icon" href=/favicon.ico><meta charset=utf-8><meta name=generator content="Hugo 0.109.0"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="Rafal Hollins"><meta name=description content><link rel=stylesheet href=/content/css/main.min.144f8f7b5768d14d9daaea0f50004cc774e28fb08b29178450f3e9b3c1975e38.css><meta name=twitter:card content="summary"><meta name=twitter:title content="Jenkins on Azure Kubernetes Cluster (AKS) - How to build container images with BuildKit, Jib, Kaniko and use Azure Files NFS, Azure Container Registry (ACR) and Azure Blob storage for caching"><meta name=twitter:description content="This is second article from the series Jenkins on Azure Kubernetes Cluster (AKS):
Jenkins on Azure Kubernetes Cluster (AKS) - How to recover Jenkins controller from Azure availability zone or region failure
Jenkins on Azure Kubernetes Cluster (AKS) - How to build container images with BuildKit, Jib, Kaniko and use Azure Files NFS, Azure Container Registry (ACR) and Azure Blob storage for caching
Table of contents
What we will cover in this article Setting up our environment Deploying Virtual Network, AKS and Jenkins Deploying Azure Files NFS share with Private Endpoint and Private DNS Zone Creating Blob Storage Account, Azure Container Registry (ACR) and Service Principle account."><meta property="og:title" content="Jenkins on Azure Kubernetes Cluster (AKS) - How to build container images with BuildKit, Jib, Kaniko and use Azure Files NFS, Azure Container Registry (ACR) and Azure Blob storage for caching"><meta property="og:description" content="This is second article from the series Jenkins on Azure Kubernetes Cluster (AKS):
Jenkins on Azure Kubernetes Cluster (AKS) - How to recover Jenkins controller from Azure availability zone or region failure
Jenkins on Azure Kubernetes Cluster (AKS) - How to build container images with BuildKit, Jib, Kaniko and use Azure Files NFS, Azure Container Registry (ACR) and Azure Blob storage for caching
Table of contents
What we will cover in this article Setting up our environment Deploying Virtual Network, AKS and Jenkins Deploying Azure Files NFS share with Private Endpoint and Private DNS Zone Creating Blob Storage Account, Azure Container Registry (ACR) and Service Principle account."><meta property="og:type" content="article"><meta property="og:url" content="https://rhollins.github.io/content/posts/jenkins-on-azure-kubernetes-cluster-aks-how-to-build-container-images-with-buildkit-jib-kaniko-and-use-azure-files-nfs-azure-container-registry-acr-and-azure-blob-storage-for-caching/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-01-07T13:31:41+00:00"><meta property="article:modified_time" content="2023-01-07T13:31:41+00:00"></head><body><header class=app-header><a href=https://rhollins.github.io/content/><img class=app-header-avatar src=/content/avatar.jpg alt="Rafal Hollins"></a>
<span class=app-header-title>Rafal Hollins</span><p>#DevOps #Azure #Kubernetes #DataEngineering #Golang #CloudArchitecture</p><div class=app-header-social><a href=https://github.com/rhollins target=_blank rel="noreferrer noopener me"><svg viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-github"><title>GitHub</title><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg></a><a href=https://twitter.com/RafalHollins target=_blank rel="noreferrer noopener me"><svg viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-twitter"><title>Twitter</title><path d="M23 3a10.9 10.9.0 01-3.14 1.53 4.48 4.48.0 00-7.86 3v1A10.66 10.66.0 013 4s-4 9 5 13a11.64 11.64.0 01-7 2c9 5 20 0 20-11.5a4.5 4.5.0 00-.08-.83A7.72 7.72.0 0023 3z"/></svg></a><a href=https://www.linkedin.com/in/rafalhollins/ target=_blank rel="noreferrer noopener me"><svg viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-linkedin"><title>linkedin</title><path d="M16 8a6 6 0 016 6v7h-4v-7a2 2 0 00-2-2 2 2 0 00-2 2v7h-4v-7a6 6 0 016-6z"/><rect x="2" y="9" width="4" height="12"/><circle cx="4" cy="4" r="2"/></svg></a></div></header><main class=app-container><article class=post><header class=post-header><h1 class=post-title>Jenkins on Azure Kubernetes Cluster (AKS) - How to build container images with BuildKit, Jib, Kaniko and use Azure Files NFS, Azure Container Registry (ACR) and Azure Blob storage for caching</h1><div class=post-meta><div><svg viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-calendar"><title>calendar</title><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg>Jan 7, 2023</div><div><svg viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-clock"><title>clock</title><circle cx="12" cy="12" r="10"/><polyline points="12 6 12 12 16 14"/></svg>11 min read</div><div><svg viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tag"><title>tag</title><path d="M20.59 13.41l-7.17 7.17a2 2 0 01-2.83.0L2 12V2h10l8.59 8.59a2 2 0 010 2.82z"/><line x1="7" y1="7" x2="7.01" y2="7"/></svg><a class=tag href=https://rhollins.github.io/content/tags/azure/>azure</a>
<a class=tag href=https://rhollins.github.io/content/tags/aks/>aks</a>
<a class=tag href=https://rhollins.github.io/content/tags/jenkins/>jenkins</a>
<a class=tag href=https://rhollins.github.io/content/tags/kaniko/>kaniko</a>
<a class=tag href=https://rhollins.github.io/content/tags/jib/>jib</a>
<a class=tag href=https://rhollins.github.io/content/tags/buildkit/>buildkit</a>
<a class=tag href=https://rhollins.github.io/content/tags/azurefiles/>azurefiles</a>
<a class=tag href=https://rhollins.github.io/content/tags/azureblob/>azureblob</a></div></div></header><div class=post-content><p><img src=../../img/3/header.jpg alt></p><p>This is second article from the series <strong>Jenkins on Azure Kubernetes Cluster (AKS)</strong>:</p><ol><li><p><a href=https://rhollins.github.io/content/posts/jenkins-on-azure-kubernetes-cluster-aks-how-to-recover-jenkins-controller-from-azure-availability-zone-or-region-failure/>Jenkins on Azure Kubernetes Cluster (AKS) - How to recover Jenkins controller from Azure availability zone or region failure</a></p></li><li><p>Jenkins on Azure Kubernetes Cluster (AKS) - How to build container images with BuildKit, Jib, Kaniko and use Azure Files NFS, Azure Container Registry (ACR) and Azure Blob storage for caching</p></li></ol><p>Table of contents</p><ul><li><a href=#what-we-will-cover-in-this-article>What we will cover in this article</a></li><li><a href=#setting-up-our-environment>Setting up our environment</a><ul><li><a href=#deploying-virtual-network-aks-and-jenkins>Deploying Virtual Network, AKS and Jenkins</a></li><li><a href=deploying-azure-files-nfs-share-with-private-endpoint-and-private-dns-zone>Deploying Azure Files NFS share with Private Endpoint and Private DNS Zone</a></li><li><a href=#creating-blob-storage-account-azure-container-registry-acr-and-service-principle-account>Creating Blob Storage Account, Azure Container Registry (ACR) and Service Principle account.</a></li></ul></li><li><a href=#building-container-image-with-buildkit-and-buildx>Building container image with BuildKit and Buildx</a><ul><li><a href=#how-to-further-improve-the-speed-of-jenkins-pipeline>How to further improve the speed of Jenkins pipeline</a></li></ul></li><li><a href=#building-container-image-using-jib>Building container image using Jib</a></li><li><a href=#building-container-image-using-kaniko>Building container image using Kaniko</a></li><li><a href=#final-thoughts>Final thoughts</a></li></ul><h2 id=what-we-will-cover-in-this-article>What we will cover in this article</h2><p>It is no longer possible to run Docker outside of Docker (Dood) in Kubernetes to build containers. In this post, we will cover three alternatives like <a href=https://github.com/moby/buildkit>BuildKit</a>, <a href=https://github.com/GoogleContainerTools/kaniko>Kaniko</a>, and <a href=https://github.com/GoogleContainerTools/jib>Jib</a> and try to integrate them with other Azure resources.</p><p>Azure Container Registry (ACR) will be used to store and cache images, one of the challenges solved in this article is the authentication since we don&rsquo;t want to use an unsecure <a href="https://learn.microsoft.com/en-us/azure/container-registry/container-registry-authentication?tabs=azure-cli#admin-account">admin user</a> but instead utilize other Azure AD authentication methods.</p><p>We will also use Azure Files NFS share which due to its pretty good <a href=https://learn.microsoft.com/en-us/azure/storage/files/storage-files-scale-targets>performance</a> and the fact that it allows <a href=https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes>ReadWriteMany</a> access mode could be potentially used to not only cache container image layers, Gradle, maven or npm dependencies but also to cache and host multiple Jenkins agents workspaces.</p><p>Azure Blob store will be used to cache image layers with Buildx CLI which currently has this functionality in <a href=https://docs.docker.com/build/building/cache/backends/azblob/>preview</a>.</p><p>Here is the summary of the components we will be using and configuring:</p><ul><li>NFS file shares in Azure Files Premium with Private Endpoint, Azure Blob store, Azure Container Registry (ACR)</li><li>AKS deployed in a subnet with Azure Container Networking Interface (CNI)</li><li>Inside AKS we will be deploying Jenkins community Helm chart and also using BuildKit, Kaniko, Jib as agent containers</li></ul><h2 id=setting-up-our-environment>Setting up our environment</h2><h3 id=deploying-virtual-network-aks-and-jenkins>Deploying Virtual Network, AKS and Jenkins</h3><p>Since we did it already in the previous article you can use it for reference <a href=https://rhollins.github.io/content/posts/jenkins-on-azure-kubernetes-cluster-aks-how-to-recover-jenkins-controller-from-azure-availability-zone-or-region-failure/>Jenkins on Azure Kubernetes Cluster (AKS) - How to recover Jenkins controller from Azure availability zone or region failure</a>. Just ensure to set AKS system nodepool to bigger size VM, for example, <strong>Standard_DS2_v2</strong> since we will be hosting there Jenkins agents as well. Also inside Jenkins Helm chart values, you can remove affinity rules because we will have one node pool and skip ingress resource since we can just forward to Jenkins service port to access UI.</p><h3 id=deploying-azure-files-nfs-share-with-private-endpoint-and-private-dns-zone>Deploying Azure Files NFS share with Private Endpoint and Private DNS Zone</h3><p>First, we will deploy Azure Files with our NFS share that will be used to host Jenkins agent&rsquo;s workspaces and where we will cache other dependencies.</p><p>Create a storage account and NFS share</p><pre><code>az storage account create -n workspacesa192 -g poc-rg -l westus2 --sku Premium_ZRS --kind FileStorage --https-only false
az storage share-rm create --storage-account workspacesa192 --enabled-protocol NFS --name &quot;aksshare&quot; --quota 100
</code></pre><p>Create a subnet for our private endpoint NIC and then the Private Endpoint itself</p><pre><code>SUBNET_ID=$(az network vnet subnet show --resource-group poc-rg --vnet-name poc-vnet --name pe-subnet --query &quot;id&quot; -o tsv)
SA_ID=$(az storage account show --resource-group poc-rg --name workspacesa192 --query &quot;id&quot; -o tsv)
az network vnet subnet update --ids $SUBNET_ID --disable-private-endpoint-network-policies

PE_ID=$(az network private-endpoint create \
  --resource-group poc-rg \
  --name &quot;workspacesa192-PrivateEndpoint&quot; \
  --location westus2 \
  --subnet $SUBNET_ID \
  --private-connection-resource-id $SA_ID\
  --group-id &quot;file&quot; \
  --connection-name &quot;workspacesa192-Connection&quot; \
  --query &quot;id&quot; -o tsv)
</code></pre><p>Create Azure Private DNS Zone which will use to access our Private Endpoint and then create a DNS record for our Azure Files endpoint.</p><pre><code>DNS_ZONE_NAME=&quot;privatelink.file.core.windows.net&quot;
VNET_ID=$(az network vnet show --resource-group poc-rg --name poc-vnet --query &quot;id&quot; -o tsv)
az network private-dns zone create --resource-group poc-rg --name $DNS_ZONE_NAME --query &quot;id&quot; -o tsv

DNS_ZONE_ID=$(az network private-dns zone show  --resource-group poc-rg --name $DNS_ZONE_NAME --query &quot;id&quot;)
az network private-dns link vnet create --resource-group poc-rg --zone-name $DNS_ZONE_NAME --name &quot;poc-vnet-DnsLink&quot; --virtual-network $VNET_ID --registration-enabled false

PE_NIC=$(az network private-endpoint show --ids $PE_ID --query &quot;networkInterfaces[0].id&quot; -o tsv)
PE_IP=$(az network nic show --ids $PE_NIC --query &quot;ipConfigurations[0].privateIpAddress&quot; -o tsv)
az network private-dns record-set a create --resource-group poc-rg --zone-name $DNS_ZONE_NAME --name workspacesa192
az network private-dns record-set a add-record --resource-group poc-rg --zone-name $DNS_ZONE_NAME --record-set-name workspacesa192 --ipv4-address $PE_IP
</code></pre><h3 id=creating-blob-storage-account-azure-container-registry-acr-and-service-principle-account>Creating Blob Storage Account, Azure Container Registry (ACR) and Service Principle account</h3><p>Let’s create Blob Storage Account and Azure Container Registry for this demo we don’t need to add a Private Endpoint like it was with NFS share.</p><pre><code>az storage account create -n blobcache222 -g poc-rg -l westus2 --kind BlobStorage --access-tier Hot
az acr create -n pocacr123 -g poc-rg --sku Basic --location westus2
</code></pre><p>For solutions like BuildKit and Jib, we will need a Service Principle account which will be used to push and pull images from ACR.</p><pre><code>ACR_NAME=pocacr123
SERVICE_PRINCIPAL_NAME=acrsp
ACR_REGISTRY_ID=$(az acr show --name $ACR_NAME --query &quot;id&quot; --output tsv)
PASSWORD=$(az ad sp create-for-rbac --name $SERVICE_PRINCIPAL_NAME --scopes $ACR_REGISTRY_ID --role acrpull --query &quot;password&quot; --output tsv)
USER_NAME=$(az ad sp list --display-name $SERVICE_PRINCIPAL_NAME --query &quot;[].appId&quot; --output tsv)
echo $USER_NAME
echo $PASSWORD
</code></pre><p>Now that the account is created we need to allow it to push and pull from our ACR.</p><pre><code>ACR_REGISTRY_ID=$(az acr show --name $ACR_NAME --query id --output tsv)
az role assignment create --assignee $USER_NAME --scope $ACR_REGISTRY_ID --role acrpull
az role assignment create --assignee $USER_NAME --scope $ACR_REGISTRY_ID --role acrpush
</code></pre><p>Kaniko doesn’t use an SP account and will be able to use Managed Identity assigned to our K8s cluster so let’s also allow MI to push and pull from ACR.</p><pre><code>export AZ_PRINCIPAL_ID=$(
  az aks show -g poc-rg -n aks-wu \
    --query &quot;identityProfile.kubeletidentity.objectId&quot; \
    --output tsv
)
az role assignment create --role acrpull --assignee $AZ_PRINCIPAL_ID --scope $ACR_REGISTRY_ID
az role assignment create --role acrpush --assignee $AZ_PRINCIPAL_ID --scope $ACR_REGISTRY_ID
</code></pre><h2 id=building-container-image-with-buildkit-and-buildx>Building container image with BuildKit and Buildx</h2><p>Now let’s try to build a container using BuildKit, first make sure you can access Jenkins by doing port-forward to the K8s services for Jenkins UI.</p><pre><code>kubectl port-forward services/jenkins 8080:8080 -n default
</code></pre><p>Before we can create our pipeline let’s locally build the buildx container and push it to our new ACR. We will use this container to run buildx CLI commands. You will need <a href=https://www.docker.com/products/docker-desktop/>Docker Desktop</a> installed on your local machine. Also, make sure you have enabled buildkit <code>"buildkit": true</code> in your docker config.</p><p>Login to ACR, build the container, and push to our repo.</p><pre><code>az acr login -n pocacr123 -g poc-rg

docker buildx build \
  --tag pocacr123.azurecr.io/buildx --push . -f - &lt;&lt;EOF
# syntax=docker/dockerfile:1
FROM docker
COPY --from=docker/buildx-bin /buildx /usr/libexec/docker/cli-plugins/docker-buildx
RUN docker buildx version
EOF
</code></pre><p>We also have to create Jenkins credentials for the Blob Storage Account key as well as a password for our Service Principle account which will be used to push the new image. If you need help <a href="https://learn.microsoft.com/en-us/azure/storage/common/storage-account-keys-manage?tabs=azure-portal">here</a> is info on how to find your Storge Account key and <a href=https://www.jenkins.io/doc/book/using/using-credentials/>here</a> how to add Jenkins secret text credentials.</p><p><img src=../../img/3/creds.JPG alt></p><p>Ok now that we have everything in place let’s create a new Jenkins pipeline with the following content and run it (the <code>--username</code> here should be your application id)</p><pre><code>podTemplate(
  inheritFrom: 'default',
  workspaceVolume: nfsWorkspaceVolume(serverAddress: 'workspacesa192.file.core.windows.net', serverPath: '/workspacesa192/aksshare', readOnly: false),
  containers: [
    containerTemplate(name: 'buildx', image: 'pocacr123.azurecr.io/buildx', ttyEnabled: true, command: 'sleep', args: '99d'),
    containerTemplate(name: 'buildkit', image: 'moby/buildkit:master', ttyEnabled: true, privileged: true, command: 'buildkitd --addr tcp://0.0.0.0:1234')
  ]) {
    node(POD_LABEL) {
        stage('Main Stage') {
            container('buildx') {
                stage('inside buildx container stage') {
                    git 'https://github.com/rhollins/example-apps.git'
                    withCredentials([string(credentialsId: 'sp_password', variable: 'SP_PASSWORD'),
                                    string(credentialsId: 'blob_storageaccount_key', variable: 'BLOB_STORAGEACCOUNT_KEY')]) {
                      sh &quot;&quot;&quot;
                        docker buildx create --name remote --driver remote tcp://localhost:1234
                        docker buildx use remote
                        docker buildx ls
                        docker login pocacr123.azurecr.io --username s910b50s-3f94-43d0-4d22-00000000 --password $SP_PASSWORD
                        docker buildx build \
                          --push \
                          --cache-from type=azblob,account_url=https://blobcache222.blob.core.windows.net/,secret_access_key=$BLOB_STORAGEACCOUNT_KEY,mode=max \
                          --cache-to type=azblob,account_url=https://blobcache222.blob.core.windows.net/,secret_access_key=$BLOB_STORAGEACCOUNT_KEY,mode=max \
                          --tag pocacr123.azurecr.io/mynewapp:mytag \
                          ./simple-dockerfile
                      &quot;&quot;&quot;
                    }
                }
            }
        }
    }
}
</code></pre><ul><li>We use <code>inheritFrom</code> to make sure our pods will inherit specific settings from the parent template. This is useful when you for example use an affininty setting that should apply to all pipelines.</li><li><code>workspaceVolume</code> will map our NFS share as a workspace directory for this particular agent. We can run multiple agents which can all mount this NFS share at the same time and store data in separate directories.</li><li><code>moby/buildkit:master</code> container to run BuildKit daemon.</li><li><code>pocacr123.azurecr.io/buildx</code> container will use buildx CLI to call BuildKit daemon as our remote builder.</li><li>buildx command will use <code>--cache-from</code> and <code>--cache-to</code> to cache our layers and also use already cached layers on subsequent runs.</li></ul><p>What you will notice is that on the next run this pipeline is much faster that’s because of caching.</p><p>To find cached image layers you can go to Blob Storage Account and then to <strong>buildkit-cache</strong> container</p><p><img src=../../img/3/cached_layers.JPG alt></p><p>You will also find your built image in the ACR repository.</p><p><img src=../../img/3/image_in_acr.JPG alt></p><p>We can also access NFS share to check our Jenkins workspace files. Deploy test pod in K8s here is the content of the <code>nfs-test-pod.yaml</code> file by running <code>kubectl apply -f nfs-test-pod.yaml</code> which will mount our share under <code>/var/nfs</code>.</p><pre><code>apiVersion: v1
kind: Pod
metadata:
  name: myapp
spec:
  containers:
  - name: myapp
    image: busybox
    command: [&quot;/bin/sh&quot;, &quot;-ec&quot;, &quot;sleep 1000&quot;]
    volumeMounts:
      - name: nfs
        mountPath: /var/nfs
  volumes:
  - name: nfs
    nfs:
      server: workspacesa192.file.core.windows.net
      path: &quot;/workspacesa192/aksshare&quot;
</code></pre><p>Once deployed just use the terminal inside this pod and there you will find Jenkins workspace files from the last build.</p><p><img src=../../img/3/nfs_content.JPG alt></p><h3 id=how-to-further-improve-the-speed-of-jenkins-pipeline>How to further improve the speed of Jenkins pipeline</h3><p>You should use Azure Private Endpoints with both Azure Container Registry and Blob Storage Account this will greatly improve network latency.</p><p>To test NFS share performance you can install the tool <a href=https://kubestr.io/>kubestr</a>.</p><pre><code>curl -LO https://github.com/kastenhq/kubestr/releases/download/v0.4.36/kubestr_0.4.36_Linux_amd64.tar.gz
tar -xvf kubestr_0.4.36_Linux_amd64.tar.gz
</code></pre><p>Then create StorageClass for NFS share.</p><pre><code>apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: azurefile-csi-nfs-specific
provisioner: file.csi.azure.com
allowVolumeExpansion: true
parameters:
  storageAccount: workspacesa192
  resourceGroup: poc-rg
  shareName: aksshare
  server: workspacesa192.file.core.windows.net
  protocol: nfs
mountOptions:
  - nconnect=8
</code></pre><p>At the end just run kubestr against this <code>StorageClass</code> and it will give you some indication of the performance, which you can then use to further tweak your builds.</p><pre><code>./kubestr fio -s nfs-sc-specific
</code></pre><p>Remember that the bigger the NFS share the better performance but also higher costs, also as always with Azure worth to check what are the resource limits.</p><p>Azure also offers an overview of NFS share performance so you can see utilization in real-time and potentially check for bottlenecks.</p><p><img src=../../img/3/nfs_mon.JPG alt></p><blockquote><p><strong>Tip:</strong> <a href=https://github.com/jenkinsci/kubernetes-plugin/blob/master/examples/dind.groovy>Here</a> is another way which utilize Docker-in-Docker in Jenkins pipeline.</p></blockquote><h2 id=building-container-image-using-jib>Building container image using Jib</h2><p>Before we run this pipeline create directory <code>/workspacesa192/aksshare/jib-test/</code> in the NFS share. It will be used to cache Gradle build dependencies. You can use the test pod we have deployed in the previous Buildx example to get access to NFS share.</p><pre><code>podTemplate(
  inheritFrom: 'default',
  workspaceVolume: nfsWorkspaceVolume(serverAddress: 'workspacesa192.file.core.windows.net', serverPath: '/workspacesa192/aksshare', readOnly: false),
  containers: [
    containerTemplate(name: 'gradle', image: 'gradle:6.4.1-jdk11', ttyEnabled: true, command: 'sleep', args: '99d'),
    containerTemplate(name: 'docker', image: 'docker', ttyEnabled: true, command: 'sleep', args: '99d')
  ],
  volumes: [
    nfsVolume(serverAddress: 'workspacesa192.file.core.windows.net', serverPath: '/workspacesa192/aksshare/jib-test/.gradle', mountPath: '/home/gradle/.gradle', readOnly: false),
    hostPathVolume(hostPath: '/root/.docker', mountPath: '/root/.docker')
  ]) {
    node(POD_LABEL) {
        stage('Main Stage') {
            container('docker') {
                stage('inside docker container stage') {
                    withCredentials([string(credentialsId: 'sp_password', variable: 'SP_PASSWORD')]) {
                      sh '''
                        docker login pocacr123.azurecr.io --username s910b50s-3f94-43d0-4d22-00000000 --password $SP_PASSWORD
                      '''
                    }
                }
            }
            container('gradle') {
                stage('inside gradle container stage') {
                    git 'https://github.com/rhollins/example-apps.git'
                    sh '''
                      cd ./gradle-spring-boot
                      gradle clean build
                      gradle jib
                    '''
                }
            }
        }
    }
}
</code></pre><ul><li>We use <code>docker</code> image to authenticate with the ACR registry.</li><li><code>hostPathVolume</code> is used to mount docker <code>config.json</code> file which contains auth token for ACR and makes it available to the Gradle container and Jib.</li><li><code>nfsVolume</code> is used to mount <code>.gradle</code> cache directory in NFS share so that we can reuse it and save time on downloading dependencies on next run.</li></ul><blockquote><p><strong>Note:</strong> It would be good idea to test running multiple pipeline instances at the same time and check if it cause any problems when they start overwriting Gradle cache simultaneously.</p></blockquote><p>Inside the NFS share we can see that Gradle successfully stored dependencies.</p><p><img src=../../img/3/gradle_cache.JPG alt></p><h2 id=building-container-image-using-kaniko>Building container image using Kaniko</h2><p>Before we run this pipeline create the directory <code>/workspacesa192/aksshare/kaniko</code> in the NFS share. It will be used to cache container images. You can use the test pod deployed in the previous Buildx example to get access to NFS share.</p><p>First, we will create a pipeline to run Kaniko warmer container which will pre-download the base image to NFS share.</p><pre><code>podTemplate(
    yaml: '''
      kind: Pod
      metadata:
        name: kaniko-warmer
      spec:
        containers:
        - name: kaniko-warmer
          image: gcr.io/kaniko-project/warmer:latest
          args: [&quot;--cache-dir=/cache&quot;,
                &quot;--image=ubuntu&quot;]
          volumeMounts:
            - name: kaniko-cache
              mountPath: /cache
        volumes:
        - name: kaniko-cache
          nfs:
            server: workspacesa192.file.core.windows.net
            path: &quot;/workspacesa192/aksshare/kaniko&quot;
'''
  ) {
  node(POD_LABEL) {
    stage('Build with Kaniko') {
      sh 'sleep 30s'
    }
    stage('Logs') {
      containerLog('kaniko-warmer')
    }
  }
}
</code></pre><p>We can verify that the layers have been downloaded successfully.</p><p><img src=../../img/3/kaniko_layers.JPG alt></p><p>Kaniko can use <a href=https://learn.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/overview>Managed Identity</a> assigned to the AKS cluster to authenticate with ACR and push the image. We have to create a configmap that contains <code>config.json</code> file with <code>crdHelper</code> definition.</p><p>Content of the file config.json</p><pre><code>{ &quot;credHelpers&quot;: { &quot;pocacr123.azurecr.io&quot;: &quot;acr-env&quot; } }
</code></pre><p>Create configmap by running this command.</p><pre><code>kubectl create configmap docker-config --from-file=./config.json
</code></pre><p>Then we can create a Jenkins pipeline and run it.</p><pre><code>podTemplate(
    volumes: [
      nfsVolume(serverAddress: 'workspacesa192.file.core.windows.net', serverPath: '/workspacesa192/aksshare/kaniko', mountPath: '/kaniko', readOnly: false)
    ],
    yaml: '''
      kind: Pod
      spec:
        containers:
        - name: kaniko
          image: gcr.io/kaniko-project/executor:v1.9.1-debug
          imagePullPolicy: Always
          command:
          - sleep
          args:
          - 99d
          volumeMounts:
            - name: docker-config
              mountPath: /kaniko/.docker/
        volumes:
          - name: docker-config
            configMap:
              name: docker-config
        restartPolicy: Never
'''
  ) {

  node(POD_LABEL) {
    stage('Build with Kaniko') {
      git 'https://github.com/rhollins/example-apps.git'
      container('kaniko') {
        sh '/kaniko/executor -f `pwd`/simple-dockerfile/Dockerfile -c `pwd` --verbosity debug --destination=pocacr123.azurecr.io/kanikoexample:mytag -cache --cache-dir=/kaniko'
      }
    }
  }
}
</code></pre><ul><li>We use <code>nfsVolume</code> to mount <code>/workspacesa192/aksshare/kaniko</code> directory which contains a cached base image.</li><li>We also mount configmap <code>docker-config</code> to instruct Kaniko how to authenticate with our ACR, in this case, it will be done by MI assigned to AKS.</li></ul><h2 id=final-thoughts>Final thoughts</h2><ul><li>We haven’t mentioned <a href=https://docs.docker.com/build/building/drivers/kubernetes/>Buildx Kubernetes</a> driver which allows you to create multiple container builders that can be run as for example K8s ad-hoc jobs to reduce cost. You can even use bilders from Dev team machines to use cloud resources.</li></ul></div><div class=post-footer></div></article></main></body></html>